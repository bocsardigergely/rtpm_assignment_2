{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:47<00:00, 657.00it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 42995/42995 [00:10<00:00, 4021.40it/s]\n"
     ]
    }
   ],
   "source": [
    "application_log = pm4py.read_xes('BPI Challenge 2017.xes')\n",
    "offer_log = pm4py.read_xes('BPI Challenge 2017 - Offer log.xes')\n",
    "\n",
    "df_application = pm4py.convert_to_dataframe(application_log)\n",
    "df_offer = pm4py.convert_to_dataframe(offer_log)\n",
    "\n",
    "df_application.to_csv('app_logs.csv')\n",
    "df_offer.to_csv('offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application = pd.read_csv('./../../Assignment_2/Data/app_logs.csv')\n",
    "df_offer = pd.read_csv('./../../Assignment_2/Data/offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "declined_ids = list(df_application.loc[df_application['concept:name'] == 'A_Denied']['case:concept:name'].unique())\n",
    "accepted_ids = list(df_application.loc[df_application['concept:name'] == 'A_Pending']['case:concept:name'].unique())\n",
    "cancelled_ids = list(df_application.loc[df_application['concept:name'] == 'A_Cancelled']['case:concept:name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31411"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 31,509 loan applications in total\n",
    "# 98 noise \n",
    "len(declined_ids)+ len(accepted_ids)+len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3752, 17228, 10431)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(declined_ids), len(accepted_ids), len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_time(ids, i):\n",
    "    timestamps = df_application.loc[df_application['case:concept:name'] == ids[i]]['time:timestamp']\n",
    "    return (min(timestamps), max(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declined_ids: 0.19175868569889576\n",
    "# cancelled_ids: 0.19131614654002713\n",
    "# approved_ids: 0.19133955289561697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    timestamp_list.append(get_min_max_time(accepted_ids, i))\n",
    "#all(timestamp_list[i] <= timestamp_list[i+1] for i in range(len(timestamp_list) - 1))\n",
    "\n",
    "accepted_ids_w_st_time = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1][0])\n",
    "accepted_ids_w_en_time = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1][1])\n",
    "\n",
    "ids_length = len(timestamp_list)\n",
    "train_num = int(ids_length * 0.8)\n",
    "\n",
    "train_ids = [pair[0] for pair in accepted_ids_w_st_time[:train_num]]\n",
    "test_ids = [pair[0] for pair in accepted_ids_w_en_time[train_num:]]\n",
    "\n",
    "overlap = list(set(train_ids) & set(test_ids))\n",
    "len(overlap)\n",
    "\n",
    "print((len(test_ids)-len(overlap))/(len(timestamp_list)-len(overlap)))\n",
    "\n",
    "test_ids = list(set(test_ids) - set(overlap))\n",
    "\n",
    "train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "train_df.to_csv('approved_train.csv')\n",
    "test_df.to_csv('approved_test.csv')\n",
    "\n",
    "#\n",
    "#train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "#pm4py.write_xes(train_event_log, 'approved_train.xes')\n",
    "#\n",
    "#test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "#pm4py.write_xes(test_event_log, 'approved_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_application, ids, outcome):\n",
    "\n",
    "    timestamp_list = []\n",
    "    for i in tqdm(range(len(ids))):\n",
    "        timestamp_list.append(get_min_max_time(ids, i))\n",
    "    #all(timestamp_list[i] <= timestamp_list[i+1] for i in range(len(timestamp_list) - 1))\n",
    "    \n",
    "    ids_w_st_time = sorted([(id, time) for id, time in zip(ids, timestamp_list)], key=lambda x: x[1][0])\n",
    "    ids_w_et_time = sorted([(id, time) for id, time in zip(ids, timestamp_list)], key=lambda x: x[1][1])\n",
    "    \n",
    "    ids_length = len(timestamp_list)\n",
    "    train_num = int(ids_length * 0.8)\n",
    "    \n",
    "    train_ids = [pair[0] for pair in ids_w_st_time[:train_num]]\n",
    "    test_ids = [pair[0] for pair in ids_w_et_time[train_num:]]\n",
    "    \n",
    "    overlap = list(set(train_ids) & set(test_ids))\n",
    "    \n",
    "    print((len(test_ids)-len(overlap))/(len(timestamp_list)-len(overlap)))\n",
    "    \n",
    "    test_ids = list(set(test_ids) - set(overlap))\n",
    "    \n",
    "    train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "    test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "    \n",
    "    train_df.to_csv(f'{outcome}_train.csv')\n",
    "    test_df.to_csv(f'{outcome}_test.csv')\n",
    "\n",
    "    return train_df, test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3752/3752 [03:54<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19175868569889576\n"
     ]
    }
   ],
   "source": [
    "d_train_df, d_test_df = create_train_test(df_application, declined_ids, 'declined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10431/10431 [10:48<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19131614654002713\n"
     ]
    }
   ],
   "source": [
    "c_train_df, c_test_df = create_train_test(df_application, cancelled_ids, 'cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17228/17228 [17:48<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19133955289561697\n"
     ]
    }
   ],
   "source": [
    "a_train_df, a_test_df = create_train_test(df_application, accepted_ids, 'approved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xes(train_df, test_df, pre):\n",
    "\n",
    "    train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "    pm4py.write_xes(train_event_log, f'{pre}_train.xes')\n",
    "\n",
    "    test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "    pm4py.write_xes(test_event_log, f'{pre}_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df.iloc[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_aggregate(result, df_row):\n",
    "    \"\"\" \n",
    "    When a new event happens, add the event info to the current aggregated result.\n",
    "\n",
    "    Input: \n",
    "         result: the current aggregated result\n",
    "         df_row: pandas df row representing the new event\n",
    "    Output:\n",
    "        result: the new aggregated result\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df_row.iloc[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO:\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    # extracting prefix for each application\n",
    "    for app_id in app_id_list:\n",
    "        \n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # A_Accepted happens at most 1 time in each case\n",
    "        # Otherwise will give error - only consider the first A_Accepted\n",
    "        cur_id = starting_row_id = events_app.loc[events_app['concept:name'] == 'A_Accepted'].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "        \n",
    "        ending_row_id = events_app.loc[events_app['concept:name'] == end_event].index[0]\n",
    "        cur_id += 1\n",
    "        \n",
    "        while cur_id < ending_row_id:\n",
    "            new_row = events_app.iloc[cur_id]\n",
    "            # TODO: \n",
    "            # add new event row info to the aggregated result\n",
    "            result = add_to_aggregate(pre_events)\n",
    "\n",
    "            # Update the return_df -> add new row\n",
    "            # target y: end_event\n",
    "\n",
    "            cur_id += 1\n",
    "\n",
    "        return return_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 building dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build function to retrieve minimum time \n",
    "def get_min_time(ids, i):\n",
    "    \"\"\"define function to retrieve the minimum time\"\"\"\n",
    "    return min(df_application.loc[df_application[\"case:concept:name\"] == ids[i]][\"time:timestamp\"])\n",
    "\n",
    "#get the minimum time for the accepted ids\n",
    "min_timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    min_timestamp_list.append(get_min_time(accepted_ids, i))\n",
    "\n",
    "#create a list with every id and the start time, and another list with every id and the end time\n",
    "accepted_ids_begin = sorted([(id, time) for id, time in zip(accepted_ids, min_timestamp_list)], key=lambda x: x[1])\n",
    "accepted_ids_end = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "#generate dataframe for begin and end times \n",
    "df_accepted_ids_time_begin = pd.DataFrame(accepted_ids_begin, columns = [\"case:concept:name\", \"begin\"])\n",
    "df_accepted_ids_time_end = pd.DataFrame(accepted_ids_end, columns = [\"case:concept:name\", \"end\"])\n",
    "\n",
    "#merge dataframes on case:concept:name\n",
    "df_accepted_timestamps = df_accepted_ids_time_begin.merge(df_accepted_ids_time_end, on = \"case:concept:name\")\n",
    "\n",
    "#keep relevant time formatting\n",
    "df_accepted_timestamps[\"begin\"] = df_accepted_timestamps[\"begin\"].map(lambda x: str(x)[:19])\n",
    "df_accepted_timestamps[\"end\"] = df_accepted_timestamps[\"end\"].map(lambda x: str(x)[:19])\n",
    "\n",
    "#create function to calculate the difference in time from a dataframe with two columns containing dates and time\n",
    "def calc_duration(end, begin):\n",
    "    \"\"\"calculate the difference in time using datetime.strptime\"\"\"\n",
    "    return (datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(begin, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "\n",
    "#empty list to gather differences\n",
    "duration = []\n",
    "\n",
    "#retrieve the difference between begin and end of the trace and add to a list \n",
    "for i in range(0, len(df_accepted_timestamps)):\n",
    "    duration.append(calc_duration(df_accepted_timestamps.iloc[i][\"end\"], df_accepted_timestamps.iloc[i][\"begin\"]))\n",
    "\n",
    "#add the time difference to the df \n",
    "df_accepted_timestamps[\"duration\"] = duration\n",
    "\n",
    "#remove all cases with case time duration 0 \n",
    "df_accepted_timestamps = df_accepted_timestamps.loc[df_accepted_timestamps[\"duration\"] > 0]\n",
    "\n",
    "#get indexes to filter outliers top and bottom 5%\n",
    "outliers_index = list(range(0, round(0.05 * len(df_accepted_timestamps)))) + list(range(round(0.95 * len(df_accepted_timestamps)), len(df_accepted_timestamps)))\n",
    "\n",
    "#sort values from small to big time difference and drop respective rows\n",
    "df_accepted_timestamps = df_accepted_timestamps.sort_values(by= \"duration\", ignore_index = True).drop(labels = outliers_index, axis = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted_timestamps_begin = df_accepted_timestamps.sort_values(by = \"begin\").reset_index(drop = True)\n",
    "df_accepted_timestamps_end = df_accepted_timestamps.sort_values(by = \"end\").reset_index(drop = True)\n",
    "\n",
    "#test set range\n",
    "begin_index_test = round(0.8 * len(df_accepted_timestamps_begin))\n",
    "begin_time_test = df_accepted_timestamps_begin.iloc[begin_index_test][\"begin\"]\n",
    "x = calc_duration(max(df_accepted_timestamps[\"end\"]), begin_time_test)\n",
    "\n",
    "#train set range\n",
    "end_index_train = round(0.8 * len(df_accepted_timestamps_end))\n",
    "end_time_train = df_accepted_timestamps_end.iloc[end_index_train][\"end\"]\n",
    "y = calc_duration(end_time_train, min(df_accepted_timestamps[\"begin\"]))\n",
    " \n",
    "total_with_overlap = x + y\n",
    "total_time = calc_duration(max(df_accepted_timestamps[\"end\"]), min(df_accepted_timestamps[\"begin\"]))\n",
    "overlap_span = total_with_overlap - total_time\n",
    "\n",
    "overlap_train = 0.8 * overlap_span\n",
    "overlap_test = 0.2 * overlap_span\n",
    "\n",
    "end_time_train_with_overlap = datetime.strptime(end_time_train, \"%Y-%m-%d %H:%M:%S\") \n",
    "date_index_train = datetime.strftime((end_time_train_datetime - timedelta(seconds = overlap_train)), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "begin_time_test_with_overlap = datetime.strptime(begin_time_test, \"%Y-%m-%d %H:%M:%S\") \n",
    "date_index_test = datetime.strftime((begin_time_test_datetime + timedelta(seconds = overlap_test)), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df_train = df_accepted_timestamps_end.loc[df_accepted_timestamps_end[\"end\"] < date_index_train]\n",
    "df_test = df_accepted_timestamps_begin.loc[df_accepted_timestamps_begin[\"begin\"] > date_index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 building dataset\n",
    "---\n",
    "\n",
    "#### First XOR - test: \n",
    "\n",
    "        A_Complete -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 8344/8344 [00:22<00:00, 370.96it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 2087/2087 [00:05<00:00, 371.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_cancel_tr = pm4py.convert_to_dataframe(pm4py.read_xes('./../../Assignment_2/Data/cancelled_train.xes'))\n",
    "df_cancel_te = pm4py.convert_to_dataframe(pm4py.read_xes('./../../Assignment_2/Data/cancelled_test.xes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancel_tr.to_csv('./../../Assignment_2/Data/cancelled_train.csv')\n",
    "df_cancel_te.to_csv('./../../Assignment_2/Data/cancelled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancel_tr = pd.read_csv('./../../Assignment_2/Data/cancelled_train.csv')\n",
    "df_cancel_te = pd.read_csv('./../../Assignment_2/Data/cancelled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Action',\n",
       " 'org:resource',\n",
       " 'concept:name',\n",
       " 'EventOrigin',\n",
       " 'EventID',\n",
       " 'lifecycle:transition',\n",
       " 'time:timestamp',\n",
       " 'FirstWithdrawalAmount',\n",
       " 'NumberOfTerms',\n",
       " 'Accepted',\n",
       " 'MonthlyCost',\n",
       " 'Selected',\n",
       " 'CreditScore',\n",
       " 'OfferedAmount',\n",
       " 'OfferID',\n",
       " 'case:LoanGoal',\n",
       " 'case:ApplicationType',\n",
       " 'case:concept:name',\n",
       " 'case:RequestedAmount']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = list(df_cancel_tr.columns)\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_attr = all_columns[all_columns.index('FirstWithdrawalAmount'):]\n",
    "\n",
    "event_attr_cat = ['org:resource', 'concept:name', 'lifecycle:transition']\n",
    "event_attr_num = ['time:timestamp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns = case_attr + ['time_to_current'] + ['last_time:timestamp']\n",
    "\n",
    "resources = list(df_cancel_tr['org:resource'].unique())\n",
    "\n",
    "df_cancel_tr['event_w_lifecycly'] = df_cancel_tr.apply(lambda row: row['concept:name'].replace(' ', '_') + '_' + row['lifecycle:transition'], axis=1)\n",
    "events = list(df_cancel_tr['concept:name'].unique())\n",
    "lifecycles = list(df_cancel_tr['lifecycle:transition'].unique())\n",
    "events_w_lifecycle = list(df_cancel_tr['event_w_lifecycly'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns.extend(events)\n",
    "result_df_columns.extend(lifecycles)\n",
    "result_df_columns.extend(events_w_lifecycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.DataFrame(columns=result_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df\n",
    "result_columns = list(return_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8344/8344 [01:39<00:00, 84.21it/s]\n"
     ]
    }
   ],
   "source": [
    "app_id_list = list(df_cancel_tr['case:concept:name'].unique())\n",
    "for app_id in tqdm(app_id_list):\n",
    "\n",
    "    events_app = df_cancel_tr.loc[df_cancel_tr['case:concept:name'] == app_id]\n",
    "    events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    starting_rows = events_app.loc[events_app['concept:name'] == 'A_Complete']\n",
    "    starting_row_ids = list(starting_rows.index)\n",
    "    \n",
    "    for starting_row_id in starting_row_ids:\n",
    "        pre_events = events_app.iloc[:starting_row_id+1]\n",
    "        \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events, result_columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df, result_columns):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "    for column in df.columns:\n",
    "        pass\n",
    "\n",
    "    # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "    # sum up time as one feature?\n",
    "\n",
    "    # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_app = df_cancel_tr.loc[df_cancel_tr['case:concept:name'] == app_id_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/m2w3gccj0hx7zslrqlw6v9z80000gn/T/ipykernel_1142/2758137824.py:1: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  start_time = np.datetime64(events_app.iloc[0]['time:timestamp'])\n",
      "/var/folders/nd/m2w3gccj0hx7zslrqlw6v9z80000gn/T/ipykernel_1142/2758137824.py:2: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  end_time = np.datetime64(events_app.iloc[-1]['time:timestamp'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.timedelta64(2743216489000,'us')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = np.datetime64(events_app.iloc[0]['time:timestamp'])\n",
    "end_time = np.datetime64(events_app.iloc[-1]['time:timestamp'])\n",
    "\n",
    "duration = end_time - start_time\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "#['time_to_current'] + ['last_time:timestamp']\n",
    "\n",
    "for column in events_app.columns:\n",
    "\n",
    "    if 'case:' in column:\n",
    "        result[column] = events_app.iloc[-1][column]\n",
    "    elif 'timestamp' in column:\n",
    "        result['last_time:timestamp'] = events_app.iloc[-1]['time:timestamp']\n",
    "        result['time_to_current'] = events_app.iloc[-1]['time:timestamp'] - events_app.iloc[0]['time:timestamp']\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_part4_first_xor(df_application, current_event='O_Created', train=True, result_columns=None):\n",
    "    \"\"\"\n",
    "    Encode original applications \n",
    "    \"\"\"\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO: get all unique activities before target_event\n",
    "    # TODO: get all unique resources before target_event\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "    # use abrove info to aggregate below\n",
    "\n",
    "    for app_id in app_id_list:\n",
    "\n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        starting_row_id = events_app.loc[events_app['concept:name'] == current_event].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "\n",
    "        # aggregate rows of events and append the timestamp of the target_event\n",
    "\n",
    "        # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "\n",
    "        # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "        # sum up time as one feature?\n",
    "\n",
    "        # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "207398d66480d5b2b9abbd127913c2d129a9922276615475dd81a0eedd9eb59f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
