{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:47<00:00, 657.00it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 42995/42995 [00:10<00:00, 4021.40it/s]\n"
     ]
    }
   ],
   "source": [
    "application_log = pm4py.read_xes('BPI Challenge 2017.xes')\n",
    "offer_log = pm4py.read_xes('BPI Challenge 2017 - Offer log.xes')\n",
    "\n",
    "df_application = pm4py.convert_to_dataframe(application_log)\n",
    "df_offer = pm4py.convert_to_dataframe(offer_log)\n",
    "\n",
    "df_application.to_csv('app_logs.csv')\n",
    "df_offer.to_csv('offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application = pd.read_csv('./../../Assignment_2/Data/app_logs.csv')\n",
    "df_offer = pd.read_csv('./../../Assignment_2/Data/offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "declined_ids = list(df_application.loc[df_application['concept:name'] == 'A_Denied']['case:concept:name'].unique())\n",
    "accepted_ids = list(df_application.loc[df_application['concept:name'] == 'A_Pending']['case:concept:name'].unique())\n",
    "cancelled_ids = list(df_application.loc[df_application['concept:name'] == 'A_Cancelled']['case:concept:name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 31,509 loan applications in total\n",
    "# 98 noise \n",
    "len(declined_ids)+ len(accepted_ids)+len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3752, 17228, 10431)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(declined_ids), len(accepted_ids), len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_time(ids, i):\n",
    "    return max(df_application.loc[df_application['case:concept:name'] == ids[i]]['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17228/17228 [17:38<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    timestamp_list.append(get_max_time(accepted_ids, i))\n",
    "#all(timestamp_list[i] <= timestamp_list[i+1] for i in range(len(timestamp_list) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 13782/13782 [00:38<00:00, 360.82it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3446/3446 [00:09<00:00, 347.63it/s]\n"
     ]
    }
   ],
   "source": [
    "accepted_ids_w_time = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "ids_length = len(accepted_ids_w_time)\n",
    "train_num = int(ids_length * 0.8)\n",
    "\n",
    "train_ids = [pair[0] for pair in accepted_ids_w_time[:train_num]]\n",
    "test_ids = [pair[0] for pair in accepted_ids_w_time[train_num:]]\n",
    "\n",
    "train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "pm4py.write_xes(train_event_log, 'approved_train.xes')\n",
    "\n",
    "test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "pm4py.write_xes(test_event_log, 'approved_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_application, ids):\n",
    "\n",
    "    timestamp_list = []\n",
    "    for i in tqdm(range(len(ids))):\n",
    "        timestamp_list.append(get_max_time(ids, i))\n",
    "    ids_w_time = sorted([(id, time) for id, time in zip(ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "    ids_length = len(ids_w_time)\n",
    "    train_num = int(ids_length * 0.8)\n",
    "\n",
    "    train_ids = ids_w_time[:train_num]\n",
    "    test_ids = ids_w_time[train_num:]\n",
    "\n",
    "    train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "    test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "    return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xes(train_df, test_df, pre):\n",
    "\n",
    "    train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "    pm4py.write_xes(train_event_log, f'{pre}_train.xes')\n",
    "\n",
    "    test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "    pm4py.write_xes(test_event_log, f'{pre}_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df.iloc[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_aggregate(result, df_row):\n",
    "    \"\"\" \n",
    "    When a new event happens, add the event info to the current aggregated result.\n",
    "\n",
    "    Input: \n",
    "         result: the current aggregated result\n",
    "         df_row: pandas df row representing the new event\n",
    "    Output:\n",
    "        result: the new aggregated result\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df_row.iloc[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO:\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    # extracting prefix for each application\n",
    "    for app_id in app_id_list:\n",
    "        \n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # A_Accepted happens at most 1 time in each case\n",
    "        # Otherwise will give error - only consider the first A_Accepted\n",
    "        cur_id = starting_row_id = events_app.loc[events_app['concept:name'] == 'A_Accepted'].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "        \n",
    "        ending_row_id = events_app.loc[events_app['concept:name'] == end_event].index[0]\n",
    "        cur_id += 1\n",
    "        \n",
    "        while cur_id < ending_row_id:\n",
    "            new_row = events_app.iloc[cur_id]\n",
    "            # TODO: \n",
    "            # add new event row info to the aggregated result\n",
    "            result = add_to_aggregate(pre_events)\n",
    "\n",
    "            # Update the return_df -> add new row\n",
    "            # target y: end_event\n",
    "\n",
    "            cur_id += 1\n",
    "\n",
    "        return return_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 building dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## similar to above, change the target y to the duration of the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 building dataset\n",
    "---\n",
    "\n",
    "#### First XOR - test: \n",
    "\n",
    "        A_Complete -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 8344/8344 [00:22<00:00, 370.96it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 2087/2087 [00:05<00:00, 371.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_cancel_tr = pm4py.convert_to_dataframe(pm4py.read_xes('./../../Assignment_2/Data/cancelled_train.xes'))\n",
    "df_cancel_te = pm4py.convert_to_dataframe(pm4py.read_xes('./../../Assignment_2/Data/cancelled_test.xes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancel_tr.to_csv('./../../Assignment_2/Data/cancelled_train.csv')\n",
    "df_cancel_te.to_csv('./../../Assignment_2/Data/cancelled_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Action',\n",
       " 'org:resource',\n",
       " 'concept:name',\n",
       " 'EventOrigin',\n",
       " 'EventID',\n",
       " 'lifecycle:transition',\n",
       " 'time:timestamp',\n",
       " 'FirstWithdrawalAmount',\n",
       " 'NumberOfTerms',\n",
       " 'Accepted',\n",
       " 'MonthlyCost',\n",
       " 'Selected',\n",
       " 'CreditScore',\n",
       " 'OfferedAmount',\n",
       " 'OfferID',\n",
       " 'case:LoanGoal',\n",
       " 'case:ApplicationType',\n",
       " 'case:concept:name',\n",
       " 'case:RequestedAmount']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = list(df_cancel_tr.columns)\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_attr = all_columns[all_columns.index('FirstWithdrawalAmount'):]\n",
    "\n",
    "event_attr_cat = ['org:resource', 'concept:name', 'lifecycle:transition']\n",
    "event_attr_num = ['time:timestamp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns = case_attr + ['time_to_current'] + ['last_time:timestamp']\n",
    "\n",
    "resources = list(df_cancel_tr['org:resource'].unique())\n",
    "\n",
    "df_cancel_tr['event_w_lifecycly'] = df_cancel_tr.apply(lambda row: row['concept:name'].replace(' ', '_') + '_' + row['lifecycle:transition'], axis=1)\n",
    "events = list(df_cancel_tr['concept:name'].unique())\n",
    "lifecycles = list(df_cancel_tr['lifecycle:transition'].unique())\n",
    "events_w_lifecycle = list(df_cancel_tr['event_w_lifecycly'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns.extend(events)\n",
    "result_df_columns.extend(lifecycles)\n",
    "result_df_columns.extend(events_w_lifecycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.DataFrame(columns=result_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df\n",
    "result_columns = list(return_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8344/8344 [01:39<00:00, 84.21it/s]\n"
     ]
    }
   ],
   "source": [
    "app_id_list = list(df_cancel_tr['case:concept:name'].unique())\n",
    "for app_id in tqdm(app_id_list):\n",
    "\n",
    "    events_app = df_cancel_tr.loc[df_cancel_tr['case:concept:name'] == app_id]\n",
    "    events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    starting_rows = events_app.loc[events_app['concept:name'] == 'A_Complete']\n",
    "    starting_row_ids = list(starting_rows.index)\n",
    "    \n",
    "    for starting_row_id in starting_row_ids:\n",
    "        pre_events = events_app.iloc[:starting_row_id+1]\n",
    "        \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events, result_columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df, result_columns):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "    for column in df.columns:\n",
    "        pass\n",
    "\n",
    "    # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "    # sum up time as one feature?\n",
    "\n",
    "    # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_app = df_cancel_tr.loc[df_cancel_tr['case:concept:name'] == app_id_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/m2w3gccj0hx7zslrqlw6v9z80000gn/T/ipykernel_1142/2758137824.py:1: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  start_time = np.datetime64(events_app.iloc[0]['time:timestamp'])\n",
      "/var/folders/nd/m2w3gccj0hx7zslrqlw6v9z80000gn/T/ipykernel_1142/2758137824.py:2: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  end_time = np.datetime64(events_app.iloc[-1]['time:timestamp'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.timedelta64(2743216489000,'us')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = np.datetime64(events_app.iloc[0]['time:timestamp'])\n",
    "end_time = np.datetime64(events_app.iloc[-1]['time:timestamp'])\n",
    "\n",
    "duration = end_time - start_time\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "#['time_to_current'] + ['last_time:timestamp']\n",
    "\n",
    "for column in events_app.columns:\n",
    "\n",
    "    if 'case:' in column:\n",
    "        result[column] = events_app.iloc[-1][column]\n",
    "    elif 'timestamp' in column:\n",
    "        result['last_time:timestamp'] = events_app.iloc[-1]['time:timestamp']\n",
    "        result['time_to_current'] = events_app.iloc[-1]['time:timestamp'] - events_app.iloc[0]['time:timestamp']\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_part4_first_xor(df_application, current_event='O_Created', train=True, result_columns=None):\n",
    "    \"\"\"\n",
    "    Encode original applications \n",
    "    \"\"\"\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO: get all unique activities before target_event\n",
    "    # TODO: get all unique resources before target_event\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "    # use abrove info to aggregate below\n",
    "\n",
    "    for app_id in app_id_list:\n",
    "\n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        starting_row_id = events_app.loc[events_app['concept:name'] == current_event].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "\n",
    "        # aggregate rows of events and append the timestamp of the target_event\n",
    "\n",
    "        # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "\n",
    "        # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "        # sum up time as one feature?\n",
    "\n",
    "        # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "207398d66480d5b2b9abbd127913c2d129a9922276615475dd81a0eedd9eb59f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
