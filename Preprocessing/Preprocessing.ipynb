{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:47<00:00, 657.00it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 42995/42995 [00:10<00:00, 4021.40it/s]\n"
     ]
    }
   ],
   "source": [
    "application_log = pm4py.read_xes('BPI Challenge 2017.xes')\n",
    "offer_log = pm4py.read_xes('BPI Challenge 2017 - Offer log.xes')\n",
    "\n",
    "df_application = pm4py.convert_to_dataframe(application_log)\n",
    "df_offer = pm4py.convert_to_dataframe(offer_log)\n",
    "\n",
    "df_application.to_csv('app_logs.csv')\n",
    "df_offer.to_csv('offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application = pd.read_csv('./../../Assignment_2/Data/app_logs.csv')\n",
    "df_offer = pd.read_csv('./../../Assignment_2/Data/offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "declined_ids = list(df_application.loc[df_application['concept:name'] == 'A_Denied']['case:concept:name'].unique())\n",
    "accepted_ids = list(df_application.loc[df_application['concept:name'] == 'A_Pending']['case:concept:name'].unique())\n",
    "cancelled_ids = list(df_application.loc[df_application['concept:name'] == 'A_Cancelled']['case:concept:name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 31,509 loan applications in total\n",
    "# 98 noise \n",
    "len(declined_ids)+ len(accepted_ids)+len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3752, 17228, 10431)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(declined_ids), len(accepted_ids), len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_time(ids, i):\n",
    "    return max(df_application.loc[df_application['case:concept:name'] == ids[i]]['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17228/17228 [17:38<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    timestamp_list.append(get_max_time(accepted_ids, i))\n",
    "#all(timestamp_list[i] <= timestamp_list[i+1] for i in range(len(timestamp_list) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 13782/13782 [00:38<00:00, 360.82it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3446/3446 [00:09<00:00, 347.63it/s]\n"
     ]
    }
   ],
   "source": [
    "accepted_ids_w_time = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "ids_length = len(accepted_ids_w_time)\n",
    "train_num = int(ids_length * 0.8)\n",
    "\n",
    "train_ids = [pair[0] for pair in accepted_ids_w_time[:train_num]]\n",
    "test_ids = [pair[0] for pair in accepted_ids_w_time[train_num:]]\n",
    "\n",
    "train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "pm4py.write_xes(train_event_log, 'approved_train.xes')\n",
    "\n",
    "test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "pm4py.write_xes(test_event_log, 'approved_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_application, ids):\n",
    "\n",
    "    timestamp_list = []\n",
    "    for i in tqdm(range(len(ids))):\n",
    "        timestamp_list.append(get_max_time(ids, i))\n",
    "    ids_w_time = sorted([(id, time) for id, time in zip(ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "    ids_length = len(ids_w_time)\n",
    "    train_num = int(ids_length * 0.8)\n",
    "\n",
    "    train_ids = ids_w_time[:train_num]\n",
    "    test_ids = ids_w_time[train_num:]\n",
    "\n",
    "    train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "    test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "    return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xes(train_df, test_df, pre):\n",
    "\n",
    "    train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "    pm4py.write_xes(train_event_log, f'{pre}_train.xes')\n",
    "\n",
    "    test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "    pm4py.write_xes(test_event_log, f'{pre}_test.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10431/10431 [10:41<00:00, 16.26it/s]\n"
     ]
    }
   ],
   "source": [
    "cancelled_tr, cancelled_te = create_train_test(df_application, cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: : 0it [00:00, ?it/s]\n",
      "exporting log, completed traces :: : 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "save_xes(cancelled_tr, cancelled_te, 'cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved_tr, approved_te = create_train_test(df_application, accepted_ids)\n",
    "save_xes(approved_tr, approved_te, 'approved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df.iloc[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_aggregate(result, df_row):\n",
    "    \"\"\" \n",
    "    When a new event happens, add the event info to the current aggregated result.\n",
    "\n",
    "    Input: \n",
    "         result: the current aggregated result\n",
    "         df_row: pandas df row representing the new event\n",
    "    Output:\n",
    "        result: the new aggregated result\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df_row.iloc[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO:\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    # extracting prefix for each application\n",
    "    for app_id in app_id_list:\n",
    "        \n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        cur_id = starting_row_id = events_app.loc[events_app['concept:name'] == 'A_Accepted'].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "        \n",
    "        ending_row_id = events_app.loc[events_app['concept:name'] == end_event].index[0]\n",
    "        cur_id += 1\n",
    "        \n",
    "        while cur_id < ending_row_id:\n",
    "            new_row = events_app.iloc[cur_id]\n",
    "            # TODO: \n",
    "            # add new event row info to the aggregated result\n",
    "            result = add_to_aggregate(pre_events)\n",
    "\n",
    "            # Update the return_df -> add new row\n",
    "            # target y: end_event\n",
    "\n",
    "            cur_id += 1\n",
    "\n",
    "        return return_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 building dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## similar to above, change the target y to the duration of the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_application.loc[df_application['case:concept:name'] == 'Application_428409768']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "def create_dataset_part4(df_application, app_ids, current_event, target_event):\n",
    "    \"\"\"\n",
    "    Encode original applications \n",
    "    \"\"\"\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO: get all unique activities before target_event\n",
    "    # TODO: get all unique resources before target_event\n",
    "    # use abrove info to aggregate below\n",
    "\n",
    "    for app_id in app_id_list:\n",
    "\n",
    "        all_events = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "\n",
    "        # aggregate rows of events and append the timestamp of the target_event\n",
    "\n",
    "        # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "\n",
    "        # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "        # sum up time as one feature?\n",
    "\n",
    "        # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "207398d66480d5b2b9abbd127913c2d129a9922276615475dd81a0eedd9eb59f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
