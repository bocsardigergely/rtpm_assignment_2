{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\boris\\OneDrive\\Bureaublad\\Datascience and Entrepreneurship\\Year 2\\Semester 1\\Real-Time process mining\\Assignment 2\\Preprocessing_final.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/boris/OneDrive/Bureaublad/Datascience%20and%20Entrepreneurship/Year%202/Semester%201/Real-Time%20process%20mining/Assignment%202/Preprocessing_final.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m application_log \u001b[39m=\u001b[39m pm4py\u001b[39m.\u001b[39;49mread_xes(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mboris\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mBureaublad\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDatascience and Entrepreneurship\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mYear 2\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSemester 1\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mReal-Time process mining\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mBPI Challenge 2017.xes.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boris/OneDrive/Bureaublad/Datascience%20and%20Entrepreneurship/Year%202/Semester%201/Real-Time%20process%20mining/Assignment%202/Preprocessing_final.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m offer_log \u001b[39m=\u001b[39m pm4py\u001b[39m.\u001b[39mread_xes(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mboris\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBureaublad\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDatascience and Entrepreneurship\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mYear 2\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSemester 1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mReal-Time process mining\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBPI Challenge 2017 - Offer log.xes.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boris/OneDrive/Bureaublad/Datascience%20and%20Entrepreneurship/Year%202/Semester%201/Real-Time%20process%20mining/Assignment%202/Preprocessing_final.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_application \u001b[39m=\u001b[39m pm4py\u001b[39m.\u001b[39mconvert_to_dataframe(application_log)\n",
      "File \u001b[1;32mc:\\Users\\boris\\Anaconda3\\lib\\site-packages\\pm4py\\read.py:48\u001b[0m, in \u001b[0;36mread_xes\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mReads an event log in the XES standard\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m    Event log\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_path):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFile does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpm4py\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobjects\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlog\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimporter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mxes\u001b[39;00m \u001b[39mimport\u001b[39;00m importer \u001b[39mas\u001b[39;00m xes_importer\n\u001b[0;32m     50\u001b[0m log \u001b[39m=\u001b[39m xes_importer\u001b[39m.\u001b[39mapply(file_path)\n",
      "\u001b[1;31mException\u001b[0m: File does not exist"
     ]
    }
   ],
   "source": [
    "application_log = pm4py.read_xes(r\"C:\\Users\\boris\\OneDrive\\Bureaublad\\Datascience and Entrepreneurship\\Year 2\\Semester 1\\Real-Time process mining\\Assignment 2\\BPI Challenge 2017.xes.gz\")\n",
    "offer_log = pm4py.read_xes(r\"C:\\Users\\boris\\OneDrive\\Bureaublad\\Datascience and Entrepreneurship\\Year 2\\Semester 1\\Real-Time process mining\\Assignment 2\\BPI Challenge 2017 - Offer log.xes.gz\")\n",
    "\n",
    "df_application = pm4py.convert_to_dataframe(application_log)\n",
    "df_offer = pm4py.convert_to_dataframe(offer_log)\n",
    "\n",
    "df_application.to_csv('app_logs.csv')\n",
    "df_offer.to_csv('offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application = pd.read_csv(r'C:\\Users\\boris\\OneDrive\\Bureaublad\\Datascience and Entrepreneurship\\Year 2\\Semester 1\\Real-Time process mining\\app_logs.csv')\n",
    "df_offer = pd.read_csv(r'C:\\Users\\boris\\OneDrive\\Bureaublad\\Datascience and Entrepreneurship\\Year 2\\Semester 1\\Real-Time process mining\\offer_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offer = df_offer.sample(frac=0.05, random_state=1)\n",
    "df_application = df_application.sample(frac=0.05, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "declined_ids = list(df_application.loc[df_application['concept:name'] == 'A_Denied']['case:concept:name'].unique())\n",
    "accepted_ids = list(df_application.loc[df_application['concept:name'] == 'A_Pending']['case:concept:name'].unique())\n",
    "cancelled_ids = list(df_application.loc[df_application['concept:name'] == 'A_Cancelled']['case:concept:name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 31,509 loan applications in total\n",
    "# 98 noise \n",
    "len(declined_ids)+ len(accepted_ids)+len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 892, 521)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(declined_ids), len(accepted_ids), len(cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_time(ids, i):\n",
    "    return max(df_application.loc[df_application['case:concept:name'] == ids[i]]['time:timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 892/892 [00:05<00:00, 153.55it/s]\n"
     ]
    }
   ],
   "source": [
    "timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    timestamp_list.append(get_max_time(accepted_ids, i))\n",
    "#all(timestamp_list[i] <= timestamp_list[i+1] for i in range(len(timestamp_list) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddba9f51a55448b99f30bb88745e3090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499e271870de457e99ee113a21d19d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accepted_ids_w_time = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "ids_length = len(accepted_ids_w_time)\n",
    "train_num = int(ids_length * 0.8)\n",
    "\n",
    "train_ids = [pair[0] for pair in accepted_ids_w_time[:train_num]]\n",
    "test_ids = [pair[0] for pair in accepted_ids_w_time[train_num:]]\n",
    "\n",
    "train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "pm4py.write_xes(train_event_log, 'approved_train.xes')\n",
    "\n",
    "test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "pm4py.write_xes(test_event_log, 'approved_test.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df_application, ids):\n",
    "\n",
    "    timestamp_list = []\n",
    "    for i in tqdm(range(len(ids))):\n",
    "        timestamp_list.append(get_max_time(ids, i))\n",
    "    ids_w_time = sorted([(id, time) for id, time in zip(ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "    ids_length = len(ids_w_time)\n",
    "    train_num = int(ids_length * 0.8)\n",
    "\n",
    "    train_ids = ids_w_time[:train_num]\n",
    "    test_ids = ids_w_time[train_num:]\n",
    "\n",
    "    train_df = df_application.loc[df_application['case:concept:name'].isin(train_ids)]\n",
    "    test_df = df_application.loc[df_application['case:concept:name'].isin(test_ids)]\n",
    "\n",
    "    return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xes(train_df, test_df, pre):\n",
    "\n",
    "    train_event_log = pm4py.convert_to_event_log(train_df)\n",
    "    pm4py.write_xes(train_event_log, f'{pre}_train.xes')\n",
    "\n",
    "    test_event_log = pm4py.convert_to_event_log(test_df)\n",
    "    pm4py.write_xes(test_event_log, f'{pre}_test.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521/521 [00:06<00:00, 77.01it/s] \n"
     ]
    }
   ],
   "source": [
    "cancelled_tr, cancelled_te = create_train_test(df_application, cancelled_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962f139d547748628c54734130f6e9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces :: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4d3bf44a57460b88d38c61cd5b8af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces :: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_xes(cancelled_tr, cancelled_te, 'cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 892/892 [00:10<00:00, 83.91it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db05f557bc9d4726bd49d9f841cbaf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces :: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82df7dc456148b4804f498f95401d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces :: : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "approved_tr, approved_te = create_train_test(df_application, accepted_ids)\n",
    "save_xes(approved_tr, approved_te, 'approved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df(df):\n",
    "    \"\"\" \n",
    "    Aggregate the df of current events in the case\n",
    "\n",
    "    Output: \n",
    "        result -> could be a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df.iloc[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_aggregate(result, df_row):\n",
    "    \"\"\" \n",
    "    When a new event happens, add the event info to the current aggregated result.\n",
    "\n",
    "    Input: \n",
    "         result: the current aggregated result\n",
    "         df_row: pandas df row representing the new event\n",
    "    Output:\n",
    "        result: the new aggregated result\n",
    "    \"\"\"\n",
    "\n",
    "    # record the timestamp of the last activity\n",
    "    result = df_row.iloc[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO:\n",
    "    # create a return df\n",
    "    return_df = pd.DataFrame()\n",
    "\n",
    "    # extracting prefix for each application\n",
    "    for app_id in app_id_list:\n",
    "        \n",
    "        events_app = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "        events_app.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        cur_id = starting_row_id = events_app.loc[events_app['concept:name'] == 'A_Accepted'].index[0]\n",
    "        pre_events = events_app.iloc[:starting_row_id]\n",
    "        # TODO: \n",
    "        # aggregate events_app from row 0 to starting_row_id\n",
    "        result = aggregate_df(pre_events)\n",
    "        \n",
    "        ending_row_id = events_app.loc[events_app['concept:name'] == end_event].index[0]\n",
    "        cur_id += 1\n",
    "        \n",
    "        while cur_id < ending_row_id:\n",
    "            new_row = events_app.iloc[cur_id]\n",
    "            # TODO: \n",
    "            # add new event row info to the aggregated result\n",
    "            result = add_to_aggregate(pre_events)\n",
    "\n",
    "            # Update the return_df -> add new row\n",
    "            # target y: end_event\n",
    "\n",
    "            cur_id += 1\n",
    "\n",
    "        return return_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 building dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 892/892 [00:12<00:00, 73.43it/s]\n"
     ]
    }
   ],
   "source": [
    "#build function to retrieve minimum time \n",
    "def get_min_time(ids, i):\n",
    "    \"\"\"define function to retrieve the minimum time\"\"\"\n",
    "    return min(df_application.loc[df_application[\"case:concept:name\"] == ids[i]][\"time:timestamp\"])\n",
    "\n",
    "#get the minimum time for the accepted ids\n",
    "min_timestamp_list = []\n",
    "for i in tqdm(range(len(accepted_ids))):\n",
    "    min_timestamp_list.append(get_min_time(accepted_ids, i))\n",
    "\n",
    "#create a list with every id and the start time, and another list with every id and the end time\n",
    "accepted_ids_begin = sorted([(id, time) for id, time in zip(accepted_ids, min_timestamp_list)], key=lambda x: x[1])\n",
    "accepted_ids_end = sorted([(id, time) for id, time in zip(accepted_ids, timestamp_list)], key=lambda x: x[1])\n",
    "\n",
    "#generate dataframe for begin and end times \n",
    "df_accepted_ids_time_begin = pd.DataFrame(accepted_ids_begin, columns = [\"case:concept:name\", \"begin\"])\n",
    "df_accepted_ids_time_end = pd.DataFrame(accepted_ids_end, columns = [\"case:concept:name\", \"end\"])\n",
    "\n",
    "#merge dataframes on case:concept:name\n",
    "df_accepted_timestamps = df_accepted_ids_time_begin.merge(df_accepted_ids_time_end, on = \"case:concept:name\")\n",
    "\n",
    "#keep relevant time formatting\n",
    "df_accepted_timestamps[\"begin\"] = df_accepted_timestamps[\"begin\"].map(lambda x: str(x)[:19])\n",
    "df_accepted_timestamps[\"end\"] = df_accepted_timestamps[\"end\"].map(lambda x: str(x)[:19])\n",
    "\n",
    "#create function to calculate the difference in time from a dataframe with two columns containing dates and time\n",
    "def calc_duration(end, begin):\n",
    "    \"\"\"calculate the difference in time using datetime.strptime\"\"\"\n",
    "    return (datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(begin, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "\n",
    "#empty list to gather differences\n",
    "duration = []\n",
    "\n",
    "#retrieve the difference between begin and end of the trace and add to a list \n",
    "for i in range(0, len(df_accepted_timestamps)):\n",
    "    duration.append(calc_duration(df_accepted_timestamps.iloc[i][\"end\"], df_accepted_timestamps.iloc[i][\"begin\"]))\n",
    "\n",
    "#add the time difference to the df \n",
    "df_accepted_timestamps[\"duration\"] = duration\n",
    "\n",
    "#remove all cases with case time duration 0 \n",
    "df_accepted_timestamps = df_accepted_timestamps.loc[df_accepted_timestamps[\"duration\"] > 0]\n",
    "\n",
    "#get indexes to filter outliers top and bottom 5%\n",
    "outliers_index = list(range(0, round(0.05 * len(df_accepted_timestamps)))) + list(range(round(0.95 * len(df_accepted_timestamps)), len(df_accepted_timestamps)))\n",
    "\n",
    "#sort values from small to big time difference and drop respective rows\n",
    "df_accepted_timestamps = df_accepted_timestamps.sort_values(by= \"duration\", ignore_index = True).drop(labels = outliers_index, axis = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-11-10 08:31:15 2016-10-28 20:54:15 2016-10-25 18:00:01 2016-10-28 20:54:15\n"
     ]
    }
   ],
   "source": [
    "df_accepted_timestamps_begin = df_accepted_timestamps.sort_values(by = \"begin\").reset_index(drop = True)\n",
    "df_accepted_timestamps_end = df_accepted_timestamps.sort_values(by = \"end\").reset_index(drop = True)\n",
    "\n",
    "#test set range\n",
    "begin_index_test = round(0.8 * len(df_accepted_timestamps_begin))\n",
    "begin_time_test = df_accepted_timestamps_begin.iloc[begin_index_test][\"begin\"]\n",
    "x = calc_duration(max(df_accepted_timestamps[\"end\"]), begin_time_test)\n",
    "\n",
    "#train set range\n",
    "end_index_train = round(0.8 * len(df_accepted_timestamps_end))\n",
    "end_time_train = df_accepted_timestamps_end.iloc[end_index_train][\"end\"]\n",
    "y = calc_duration(end_time_train, min(df_accepted_timestamps[\"begin\"]))\n",
    " \n",
    "total_with_overlap = x + y\n",
    "total_time = calc_duration(max(df_accepted_timestamps[\"end\"]), min(df_accepted_timestamps[\"begin\"]))\n",
    "overlap_span = total_with_overlap - total_time\n",
    "\n",
    "overlap_train = 0.8 * overlap_span\n",
    "overlap_test = 0.2 * overlap_span\n",
    "\n",
    "end_time_train_with_overlap = datetime.strptime(end_time_train, \"%Y-%m-%d %H:%M:%S\") \n",
    "date_index_train = datetime.strftime((end_time_train_datetime - timedelta(seconds = overlap_train)), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "begin_time_test_with_overlap = datetime.strptime(begin_time_test, \"%Y-%m-%d %H:%M:%S\") \n",
    "date_index_test = datetime.strftime((begin_time_test_datetime + timedelta(seconds = overlap_test)), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df_train = df_accepted_timestamps_end.loc[df_accepted_timestamps_end[\"end\"] < date_index_train]\n",
    "df_test = df_accepted_timestamps_begin.loc[df_accepted_timestamps_begin[\"begin\"] > date_index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_application.loc[df_application['case:concept:name'] == 'Application_428409768']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prefix_part2(df_application, app_ids, end_event, start_event='A_Accepted'):\n",
    "def create_dataset_part4(df_application, app_ids, current_event, target_event):\n",
    "    \"\"\"\n",
    "    Encode original applications \n",
    "    \"\"\"\n",
    "    \n",
    "    app_id_list = list(df_application['case:concept:name'].unique())\n",
    "\n",
    "    # TODO: get all unique activities before target_event\n",
    "    # TODO: get all unique resources before target_event\n",
    "    # use abrove info to aggregate below\n",
    "\n",
    "    for app_id in app_id_list:\n",
    "\n",
    "        all_events = df_application.loc[df_application['case:concept:name'] == app_id]\n",
    "\n",
    "        # aggregate rows of events and append the timestamp of the target_event\n",
    "\n",
    "        # Encode each case attribute as a feature (or one-hot encode categorical case attributes)\n",
    "\n",
    "        # For each numerical event attribute, apply an aggregation function (e.g. average) over the sequence of values taken by this attribute in the prefix\n",
    "        # sum up time as one feature?\n",
    "\n",
    "        # For each categorical event attribute, encode each possible value of that attribute as a numerical feature. \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdfd0ab590d37ad3e186c66b82c017e01055655de06222048609fc9f6dee6823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
